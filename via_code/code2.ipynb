{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a926943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_20736\\4194890209.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting schemes into text chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting: 100%|██████████| 3397/3397 [00:01<00:00, 2055.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split 3397 schemes into 45243 text chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\scheme_data.csv\")\n",
    "df.fillna(\"\", inplace=True)\n",
    "df = df.drop_duplicates(subset=[\"slug\"])\n",
    "\n",
    "# Combine all relevant info into pre-formatted content per scheme\n",
    "def format_content(row):\n",
    "    return f\"\"\"\n",
    "Scheme Name: {row['scheme_name']}\n",
    "Level: {row['level']}\n",
    "Category: {row['schemeCategory']}\n",
    "Tags: {row['tags']}\n",
    "\n",
    "Details:\n",
    "{row['details']}\n",
    "\n",
    "Benefits:\n",
    "{row['benefits']}\n",
    "\n",
    "Eligibility:\n",
    "{row['eligibility']}\n",
    "\n",
    "Application Process:\n",
    "{row['application']}\n",
    "\n",
    "Documents Required:\n",
    "{row['documents']}\n",
    "\"\"\"\n",
    "\n",
    "df[\"content\"] = df.apply(format_content, axis=1)\n",
    "\n",
    "# Create metadata dictionary\n",
    "df[\"metadata\"] = df.apply(lambda row: {\n",
    "    \"scheme_name\": row[\"scheme_name\"],\n",
    "    \"slug\": row[\"slug\"],\n",
    "    \"level\": row[\"level\"],\n",
    "    \"category\": row[\"schemeCategory\"],\n",
    "    \"tags\": row[\"tags\"]\n",
    "}, axis=1)\n",
    "\n",
    "# Split content into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,   # adjusted for GPT-Neo-125M context\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "documents = []\n",
    "print(\"Splitting schemes into text chunks...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Splitting\"):\n",
    "    chunks = text_splitter.split_text(row[\"content\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk.strip():\n",
    "            documents.append({\n",
    "                \"id\": f\"{row['slug']}_chunk{i+1}\",\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": row[\"metadata\"]\n",
    "            })\n",
    "\n",
    "print(f\"✅ Split {len(df)} schemes into {len(documents)} text chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56673f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and adding documents in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 91/91 [08:01<00:00,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chroma vector store created with 45243 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_20736\\3294312021.py:31: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert to LangChain Document objects\n",
    "docs_with_metadata = [\n",
    "    Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"])\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Initialize empty Chroma store\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"schemes_db\",\n",
    "    persist_directory=\"./chroma_store3\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Batch size for embedding + adding\n",
    "batch_size = 500\n",
    "print(\"Embedding and adding documents in batches...\")\n",
    "\n",
    "for i in tqdm(range(0, len(docs_with_metadata), batch_size), desc=\"Embedding batches\"):\n",
    "    batch = docs_with_metadata[i : i + batch_size]\n",
    "    vectordb.add_documents(batch)\n",
    "\n",
    "# Persist DB\n",
    "vectordb.persist()\n",
    "print(f\"✅ Chroma vector store created with {len(documents)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a63b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-Neo-125M pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MISTY ROY\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-1.3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query: Tell me all information about Prime Minister’s Fellowship for Doctoral Research\n",
      "\n",
      "Answer:\n",
      " \n",
      "You are a government schemes assistant.\n",
      "Answer ONLY using the provided context.\n",
      "If information is missing, write \"Not available\".\n",
      "\n",
      "Context:\n",
      "on industrially relevant subject areas. Key Features of the Fellowship: In addition to the attractive scholarship, the Prime Minister’s Fellowship emphasizes providing a unique and invigorating experience to selected fellows. It ensures the best national and international exposure for them and provides mentoring through industry and academic experts through the mechanism of annual review meetings. In addition, periodic mentorship sessions are also organized with the help of expert external\n",
      "\n",
      "Prime Minister’s Fellowship for Doctoral Research scheme is a prestigious initiative of the Science and Engineering Research Board (SERB), Department of Science & Technology, Government of India towards the advancement of university research engagements in line with industry requirements. This scheme is aimed at encouraging young, talented, enthusiastic, and result-oriented scholars to take up industry-relevant research by partnering with institutions of academic excellence. The intent was to\n",
      "\n",
      "Scheme Name: Prime Minister’s Fellowship for Doctoral Research\n",
      "Level: Central\n",
      "Category: Education & Learning, Science, IT & Communications\n",
      "Tags: Fellowship, Doctoral, Research, SERB, PhD, Fellow\n",
      "\n",
      "etc. The Post-Doctoral Fellowship can be availed for research in an institution other than the department or institute in India where the candidate has undergone Ph.D. work. Objectives of the scheme: To support the research aptitude of Doctorate holders in Science & Technology. To motivate Ph. D holders to pursue research and to develop careers as scientists. To motivate fellows and impart the skills needed for them to become lifelong learners and to develop technical and leadership skills.\n",
      "\n",
      "Application Process: Step 01: The Ministry of Culture and/or the concerned institution will widely advertise the Fellowship in the leading national/regional newspapers and on its website (which should give all details) and also disseminate the scheme through professional associations/forums in the relevant fields, so that maximum publicity is accorded to the Scheme. Step 02: The eligible fellows who can spare time of about two years to do a project based on the resources of any of the\n",
      "\n",
      "User Question:\n",
      "Tell me all information about Prime Minister’s Fellowship for Doctoral Research\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "{\n",
      "  \"Scheme Name\": \"...\",\n",
      "  \"Eligibility\": \"...\",\n",
      "  \"Benefits\": \"...\",\n",
      "  \"Application Process\": \"...\",\n",
      "  \"Required Documents\": \"...\",\n",
      "  \"Validity / Duration\": \"...\",\n",
      "  \"Level\": \"...\",\n",
      "  \"Category\": \"...\",\n",
      "  \"Tags\": \"...\"\n",
      "}\n",
      "\n",
      "Step 1\n",
      "\n",
      "Step 2\n",
      "\n",
      "Step 3\n",
      "\n",
      "Step 4\n",
      "\n",
      "Step 5\n",
      "\n",
      "Step 6\n",
      "\n",
      "Step 7\n",
      "\n",
      "Step 8\n",
      "\n",
      "Step 9\n",
      "\n",
      "Step 10\n",
      "\n",
      "Step 11\n",
      "\n",
      "Step 12\n",
      "\n",
      "Step 13\n",
      "\n",
      "Step 14\n",
      "\n",
      "Step 15\n",
      "\n",
      "Step 16\n",
      "\n",
      "Step 17\n",
      "\n",
      "Step 18\n",
      "\n",
      "Step 19\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "  \"Eligibility\": \"\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "{\"Eligibility\": \"I have completed an examination for admission to the department or institution\n",
      "for which I am applying, I assure you that I possess all necessary credentials\n",
      "and I can put forth in my application the quality of research. I am in good\n",
      "health, and I have all the facilities required for the work.\"}\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "{\n",
      "  \"Eligibility\": \"I have completed an examination for admission to\n",
      "the department or institution for which I am applying, I assure you that\n",
      "I possess all necessary credentials and I can put forth in my application\n",
      "the quality of research. I am in good health, and I have all the facilities\n",
      "required for the work.\"}\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "\"Eligibility\": \"I have completed an examination for admission to the\n",
      "department or institution for which I am applying, I assure you that I\n",
      "possess all necessary credentials and I can put forth in my application\n",
      "the quality of research. I am in good health, and I have all the facilities\n",
      "required for the work.\"}\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "{\n",
      "  \"Eligibility\": \"I have completed an examination for admission to\n",
      "the department or institution for which I am applying, I assure you that\n",
      "I possess all necessary credentials and I can put forth in my application\n",
      "the quality of research. I am in good health, and I have all the facilities\n",
      "required for the work.\"}\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "  \"Eligibility\": \"I have completed an examination for admission to the\n",
      "department or institution for which I am applying, I assure you that I\n",
      "possess all necessary credentials and I can put forth in my application\n",
      "the quality of research. I am in good health, and I have all the facilities\n",
      "required for the work.\"}\n",
      "\n",
      "Answer in this exact JSON format:\n",
      "  \"Eligibility\": \"I\n",
      "\n",
      "Sources:\n",
      "- Prime Minister’s Fellowship for Doctoral Research : on industrially relevant subject areas. Key Features of the Fellowship: In addition to the attractive scholarship, the Prime Minister’s Fellowship emphasizes providing a unique and invigorating experi ...\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Prime Minister’s Fellowship for Doctoral Research scheme is a prestigious initiative of the Science and Engineering Research Board (SERB), Department of Science & Technology, Government of India towar ...\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Scheme Name: Prime Minister’s Fellowship for Doctoral Research\n",
      "Level: Central\n",
      "Category: Education & Learning, Science, IT & Communications\n",
      "Tags: Fellowship, Doctoral, Research, SERB, PhD, Fellow ...\n",
      "- KSCSTE Post-Doctoral Fellowship Programme : etc. The Post-Doctoral Fellowship can be availed for research in an institution other than the department or institute in India where the candidate has undergone Ph.D. work. Objectives of the scheme:  ...\n",
      "- Tagore National Fellowship for Cultural Research : Application Process: Step 01: The Ministry of Culture and/or the concerned institution will widely advertise the Fellowship in the leading national/regional newspapers and on its website (which should ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "\n",
    "# 1. Reload embeddings (must match stored DB)\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Load Chroma vector DB\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"schemes_db\",\n",
    "    persist_directory=\"./chroma_store3\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# 3. Prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template = \"\"\"\n",
    "You are a government schemes assistant.\n",
    "Answer ONLY using the provided context.\n",
    "If information is missing, write \"Not available\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\n",
    "Answer in this exact JSON format:\n",
    "{{\n",
    "  \"Scheme Name\": \"...\",\n",
    "  \"Eligibility\": \"...\",\n",
    "  \"Benefits\": \"...\",\n",
    "  \"Application Process\": \"...\",\n",
    "  \"Required Documents\": \"...\",\n",
    "  \"Validity / Duration\": \"...\",\n",
    "  \"Level\": \"...\",\n",
    "  \"Category\": \"...\",\n",
    "  \"Tags\": \"...\"\n",
    "}}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Load local GPT-Neo model\n",
    "print(\"Loading GPT-Neo-125M pipeline...\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"EleutherAI/gpt-neo-1.3B\",\n",
    "    device=-1,           # -1 for CPU, 0 for GPU\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 5. RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 6. Query example\n",
    "query = \"Tell me all information about Prime Minister’s Fellowship for Doctoral Research\"\n",
    "print(f\"Running query: {query}\")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"\\nAnswer:\\n\", result['result'])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(\"-\", doc.metadata['scheme_name'], \":\", doc.page_content[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a7bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
