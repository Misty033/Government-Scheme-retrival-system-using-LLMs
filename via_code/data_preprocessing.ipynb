{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b150790f",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee148b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with 3397 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19380\\2590493965.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# --------------------------\n",
    "# Load your CSV\n",
    "# --------------------------\n",
    "df = pd.read_csv(r\"C:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\scheme_data.csv\")\n",
    "df.fillna(\"\", inplace=True)\n",
    "df = df.drop_duplicates(subset=[\"slug\"])\n",
    "\n",
    "# Combine relevant columns into one text field\n",
    "df[\"content\"] = (\n",
    "    df[\"details\"] + \"\\n\\n\" +\n",
    "    \"Benefits: \" + df[\"benefits\"] + \"\\n\\n\" +\n",
    "    \"Eligibility: \" + df[\"eligibility\"] + \"\\n\\n\" +\n",
    "    \"Application: \" + df[\"application\"] + \"\\n\\n\" +\n",
    "    \"Documents Required: \" + df[\"documents\"]\n",
    ")\n",
    "\n",
    "# Add metadata\n",
    "df[\"metadata\"] = df.apply(lambda row: {\n",
    "    \"scheme_name\": row[\"scheme_name\"],\n",
    "    \"slug\": row[\"slug\"],\n",
    "    \"level\": row[\"level\"],\n",
    "    \"category\": row[\"schemeCategory\"],\n",
    "    \"tags\": row[\"tags\"]\n",
    "}, axis=1)\n",
    "\n",
    "print(f\"Loaded CSV with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97232691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split into 33404 text chunks\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Split long text into smaller chunks\n",
    "# --------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,    # max characters per chunk\n",
    "    chunk_overlap=50   # overlap between chunks\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    chunks = text_splitter.split_text(row[\"content\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk.strip():  # skip empty chunks\n",
    "            documents.append({\n",
    "                \"id\": f\"{row['slug']}_chunk{i+1}\",\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": row[\"metadata\"]\n",
    "            })\n",
    "\n",
    "print(f\"✅ Split into {len(documents)} text chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15370135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MISTY ROY\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local embeddings working, vector length: 384\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Initialize local embeddings\n",
    "# --------------------------\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"  # small, fast, high-quality local model\n",
    ")\n",
    "\n",
    "# # Optional test\n",
    "# test_vector = embeddings.embed_documents([\"Hello world\"])\n",
    "# print(f\"Local embeddings working, vector length: {len(test_vector[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc2fdc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents with tqdm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 33404/33404 [10:23<00:00, 53.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chroma vector store created with 33404 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19380\\1055820960.py:26: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Create Chroma vector store\n",
    "# --------------------------\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Prepare empty list for Document objects\n",
    "docs_with_vectors = []\n",
    "\n",
    "print(\"Embedding documents with tqdm...\")\n",
    "for doc in tqdm(documents, desc=\"Embedding\"):\n",
    "    vector = embeddings.embed_documents([doc[\"text\"]])[0]  # embed single chunk\n",
    "    docs_with_vectors.append(\n",
    "        Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"])\n",
    "    )\n",
    "\n",
    "# Create Chroma vector store using precomputed embeddings\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs_with_vectors,\n",
    "    embedding=embeddings,\n",
    "    ids=[doc[\"id\"] for doc in documents],\n",
    "    collection_name=\"schemes_db\",\n",
    "    persist_directory=\"./chroma_store\"\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(f\"Chroma vector store created with {len(documents)} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''better code'''\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from langchain.schema import Document\n",
    "\n",
    "# # Precompute embeddings\n",
    "# docs_with_vectors = []\n",
    "# vectors = []\n",
    "\n",
    "# print(\"Embedding documents with tqdm...\")\n",
    "# for doc in tqdm(documents, desc=\"Embedding\"):\n",
    "#     vec = embeddings.embed_documents([doc[\"text\"]])[0]\n",
    "#     vectors.append(vec)\n",
    "#     docs_with_vectors.append(\n",
    "#         Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"])\n",
    "#     )\n",
    "\n",
    "# # Pass precomputed embeddings to Chroma\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=docs_with_vectors,\n",
    "#     embedding=None,  # already embedded\n",
    "#     ids=[doc[\"id\"] for doc in documents],\n",
    "#     collection_name=\"schemes_db\",\n",
    "#     persist_directory=\"./chroma_store\"\n",
    "# )\n",
    "\n",
    "# vectordb._collection.add(\n",
    "#     documents=[doc.page_content for doc in docs_with_vectors],\n",
    "#     metadatas=[doc.metadata for doc in docs_with_vectors],\n",
    "#     ids=[doc[\"id\"] for doc in documents],\n",
    "#     embeddings=vectors\n",
    "# )\n",
    "\n",
    "# vectordb.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1783033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19380\\1825388878.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load persisted Chroma vector store\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"./chroma_store\",\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"schemes_db\"\n",
    ")\n",
    "\n",
    "# Create a retriever for fetching relevant chunks\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})  # fetch top 3 relevant chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6833ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a friendly and knowledgeable assistant specialized in Indian government schemes.\n",
    "\n",
    "1. Greet the user first with: \"Hello! How may I assist you with government schemes today?\"\n",
    "\n",
    "2. Behavior:\n",
    "   - If the user's question is about one or more schemes listed in the provided context:\n",
    "       - Provide a **structured answer for each relevant scheme**.\n",
    "       - Include the following metadata for every scheme if available:\n",
    "           - **Scheme Name**  \n",
    "           - **Eligibility**  \n",
    "           - **Benefits**  \n",
    "           - **Application Process**  \n",
    "           - **Required Documents**  \n",
    "           - **Validity / Duration**  \n",
    "           - **Level (Central/State)**  \n",
    "           - **Scheme Category**  \n",
    "           - **Tags**  \n",
    "       - Present each scheme clearly, like a **numbered mini-report**.\n",
    "       - If a field is missing, mention \"Not available\".\n",
    "\n",
    "   - If the user's question is **not related** to any scheme in the context:\n",
    "       - Respond politely: \"Certainly, I don't know the answer to that. Please enter a valid question related to government schemes.\"\n",
    "\n",
    "3. Always base your answer **only on the provided context**.  \n",
    "   - Do not make up information.\n",
    "   - If the context does not have enough details, say: \"Not enough information.\"\n",
    "\n",
    "Here is the information you can use:\n",
    "\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Answer in structured, multi-scheme format:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc412244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MISTY ROY\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19380\\1370460652.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize local transformer LLM\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-small\",  # CPU-friendly local model\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create RetrievalQA chain with prompt template\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",  # concatenates retrieved chunks\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1e2c146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19380\\1600223897.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " \n",
      "You are a friendly and knowledgeable assistant specialized in Indian government schemes.\n",
      "\n",
      "1. Greet the user first with: \"Hello! How may I assist you with government schemes today?\"\n",
      "\n",
      "2. Behavior:\n",
      "   - If the user's question is about one or more schemes listed in the provided context:\n",
      "       - Provide a **structured answer for each relevant scheme**.\n",
      "       - Include the following metadata for every scheme if available:\n",
      "           - **Scheme Name**  \n",
      "           - **Eligibility**  \n",
      "           - **Benefits**  \n",
      "           - **Application Process**  \n",
      "           - **Required Documents**  \n",
      "           - **Validity / Duration**  \n",
      "           - **Level (Central/State)**  \n",
      "           - **Scheme Category**  \n",
      "           - **Tags**  \n",
      "       - Present each scheme clearly, like a **numbered mini-report**.\n",
      "       - If a field is missing, mention \"Not available\".\n",
      "\n",
      "   - If the user's question is **not related** to any scheme in the context:\n",
      "       - Respond politely: \"Certainly, I don't know the answer to that. Please enter a valid question related to government schemes.\"\n",
      "\n",
      "3. Always base your answer **only on the provided context**.  \n",
      "   - Do not make up information.\n",
      "   - If the context does not have enough details, say: \"Not enough information.\"\n",
      "\n",
      "Here is the information you can use:\n",
      "\n",
      "on industrially relevant subject areas. Key Features of the Fellowship: In addition to the attractive scholarship, the Prime Minister’s Fellowship emphasizes providing a unique and invigorating experience to selected fellows. It ensures the best national and international exposure for them and provides mentoring through industry and academic experts through the mechanism of annual review meetings. In addition, periodic mentorship sessions are also organized with the help of expert external\n",
      "\n",
      "Prime Minister’s Fellowship for Doctoral Research scheme is a prestigious initiative of the Science and Engineering Research Board (SERB), Department of Science & Technology, Government of India towards the advancement of university research engagements in line with industry requirements. This scheme is aimed at encouraging young, talented, enthusiastic, and result-oriented scholars to take up industry-relevant research by partnering with institutions of academic excellence. The intent was to\n",
      "\n",
      "Application: Application Process: Step 01: The Ministry of Culture and/or the concerned institution will widely advertise the Fellowship in the leading national/regional newspapers and on its website (which should give all details) and also disseminate the scheme through professional associations/forums in the relevant fields, so that maximum publicity is accorded to the Scheme. Step 02: The eligible fellows who can spare time of about two years to do a project based on the resources of any of\n",
      "\n",
      "User Question: Tell me all information about Prime Minister’s Fellowship for Doctoral Research\n",
      "\n",
      "Answer in structured, multi-scheme format:\n",
      "\n",
      "\n",
      "Sources:\n",
      "- Prime Minister’s Fellowship for Doctoral Research : on industrially relevant subject areas. Key Features of the Fellowship: In addition to the attractive scholarship, the Prime Minister’s Fellowship emphasizes providing a unique and invigorating experi ...\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Prime Minister’s Fellowship for Doctoral Research scheme is a prestigious initiative of the Science and Engineering Research Board (SERB), Department of Science & Technology, Government of India towar ...\n",
      "- Tagore National Fellowship for Cultural Research : Application: Application Process: Step 01: The Ministry of Culture and/or the concerned institution will widely advertise the Fellowship in the leading national/regional newspapers and on its website  ...\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me all information about Prime Minister’s Fellowship for Doctoral Research\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"Answer:\\n\", result['result'])\n",
    "\n",
    "print(\"\\nSources:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(\"-\", doc.metadata['scheme_name'], \":\", doc.page_content[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52429443",
   "metadata": {},
   "source": [
    "## Basic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_23416\\384745899.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split 3397 schemes into 54772 text chunks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\scheme_data.csv\")\n",
    "df.fillna(\"\", inplace=True)\n",
    "df = df.drop_duplicates(subset=[\"slug\"])\n",
    "\n",
    "# Combine all relevant info into pre-formatted content per scheme\n",
    "def format_content(row):\n",
    "    return f\"\"\"\n",
    "Scheme Name: {row['scheme_name']}\n",
    "Level: {row['level']}\n",
    "Category: {row['schemeCategory']}\n",
    "Tags: {row['tags']}\n",
    "\n",
    "Details:\n",
    "{row['details']}\n",
    "\n",
    "Benefits:\n",
    "{row['benefits']}\n",
    "\n",
    "Eligibility:\n",
    "{row['eligibility']}\n",
    "\n",
    "Application Process:\n",
    "{row['application']}\n",
    "\n",
    "Documents Required:\n",
    "{row['documents']}\n",
    "\"\"\"\n",
    "\n",
    "df[\"content\"] = df.apply(format_content, axis=1)\n",
    "\n",
    "# Create metadata dictionary\n",
    "df[\"metadata\"] = df.apply(lambda row: {\n",
    "    \"scheme_name\": row[\"scheme_name\"],\n",
    "    \"slug\": row[\"slug\"],\n",
    "    \"level\": row[\"level\"],\n",
    "    \"category\": row[\"schemeCategory\"],\n",
    "    \"tags\": row[\"tags\"]\n",
    "}, axis=1)\n",
    "\n",
    "# Split content into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    chunks = text_splitter.split_text(row[\"content\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if chunk.strip():\n",
    "            documents.append({\n",
    "                \"id\": f\"{row['slug']}_chunk{i+1}\",\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": row[\"metadata\"]\n",
    "            })\n",
    "\n",
    "print(f\"✅ Split {len(df)} schemes into {len(documents)} text chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a240629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_23416\\813160101.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Initialize local embeddings\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8b2307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding chunks with tqdm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 55/55 [05:35<00:00,  6.10s/it]\n",
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_23416\\233812563.py:29: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents to Chroma in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches: 100%|██████████| 11/11 [00:31<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chroma vector store created with 54772 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_23416\\233812563.py:48: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Precompute embeddings in batches\n",
    "# --------------------------\n",
    "docs_with_vectors = []\n",
    "vectors = []\n",
    "\n",
    "batch_size = 1000  # adjust based on memory/speed\n",
    "print(\"Embedding chunks with tqdm...\")\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size), desc=\"Embedding batches\"):\n",
    "    batch_docs = documents[i:i+batch_size]\n",
    "    texts = [doc[\"text\"] for doc in batch_docs]\n",
    "    \n",
    "    # Embed the batch\n",
    "    batch_vectors = embeddings.embed_documents(texts)\n",
    "    vectors.extend(batch_vectors)\n",
    "    \n",
    "    # Create Document objects\n",
    "    for doc in batch_docs:\n",
    "        docs_with_vectors.append(Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"]))\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Create Chroma collection with precomputed embeddings in batches\n",
    "# --------------------------\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"schemes_db\",       \n",
    "    persist_directory=\"./chroma_store2\",\n",
    "    embedding_function=None              \n",
    ")\n",
    "\n",
    "add_batch_size = 5000 \n",
    "print(\"Adding documents to Chroma in batches...\")\n",
    "\n",
    "for i in tqdm(range(0, len(docs_with_vectors), add_batch_size), desc=\"Adding batches\"):\n",
    "    batch_docs = docs_with_vectors[i:i+add_batch_size]\n",
    "    vectordb._collection.add(\n",
    "        documents=[doc.page_content for doc in batch_docs],\n",
    "        metadatas=[doc.metadata for doc in batch_docs],\n",
    "        ids=[doc.metadata[\"slug\"] + f\"_chunk{j+1}\" for j, doc in enumerate(batch_docs)],  # ensure unique IDs\n",
    "        embeddings=vectors[i:i+add_batch_size]\n",
    "    )\n",
    "\n",
    "# Persist the vector store\n",
    "vectordb.persist()\n",
    "print(f\"✅ Chroma vector store created with {len(documents)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd86c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " \n",
      "You are a friendly and knowledgeable assistant specialized in Indian government schemes.\n",
      "\n",
      "1. Greet the user first with: \"Hello! How may I assist you with government schemes today?\"\n",
      "\n",
      "2. Behavior:\n",
      "   - If the user's question is about one or more schemes listed in the provided context:\n",
      "       - Provide a **structured answer for each relevant scheme**.\n",
      "       - Include the following metadata for every scheme if available:\n",
      "           - **Scheme Name**  \n",
      "           - **Eligibility**  \n",
      "           - **Benefits**  \n",
      "           - **Application Process**  \n",
      "           - **Required Documents**  \n",
      "           - **Validity / Duration**  \n",
      "           - **Level (Central/State)**  \n",
      "           - **Scheme Category**  \n",
      "           - **Tags**  \n",
      "       - Present each scheme clearly, like a **numbered mini-report**.\n",
      "       - If a field is missing, mention \"Not available\".\n",
      "\n",
      "   - If the user's question is **not related** to any scheme in the context:\n",
      "       - Respond politely: \"Certainly, I don't know the answer to that. Please enter a valid question related to government schemes.\"\n",
      "\n",
      "3. Always base your answer **only on the provided context**.  \n",
      "   - Do not make up information.\n",
      "   - If the context does not have enough details, say: \"Not enough information.\"\n",
      "\n",
      "Here is the information you can use:\n",
      "\n",
      "Features of the Fellowship: In addition to the attractive scholarship, the Prime Minister’s Fellowship emphasizes providing a unique and invigorating experience to selected fellows. It ensures the best national and international exposure for them and provides mentoring through industry and academic experts through the mechanism of annual review meetings. In addition, periodic mentorship sessions\n",
      "\n",
      "Prime Minister’s Fellowship for Doctoral Research scheme is a prestigious initiative of the Science and Engineering Research Board (SERB), Department of Science & Technology, Government of India towards the advancement of university research engagements in line with industry requirements. This scheme is aimed at encouraging young, talented, enthusiastic, and result-oriented scholars to take up\n",
      "\n",
      "Scheme Name: Prime Minister’s Fellowship for Doctoral Research\n",
      "Level: Central\n",
      "Category: Education & Learning, Science, IT & Communications\n",
      "Tags: Fellowship, Doctoral, Research, SERB, PhD, Fellow\n",
      "\n",
      "the quality of the proposal, presentation, discussion, details of publications, academic records/recognitions, etc. The Post-Doctoral Fellowship can be availed for research in an institution other than the department or institute in India where the candidate has undergone Ph.D. work. Objectives of the scheme: To support the research aptitude of Doctorate holders in Science & Technology. To\n",
      "\n",
      "institutions in the country, so as to transfer fundamental research output in viable technological products/devices/components/processes. In brief, the principal objective of the Fellowship is to achieve excellence in engineering, innovation, and technology development. This fellowship scheme has been named after the late Dr. A. P. J. Abdul Kalam, the former President of India, who was a role\n",
      "\n",
      "User Question: Tell me all information about Prime Minister’s Fellowship for Doctoral Research\n",
      "\n",
      "Answer in structured, multi-scheme format:\n",
      "\n",
      "\n",
      "Sources:\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Features of the Fellowship: In addition to the attractive scholarship, the Prime Minister’s Fellowship emphasizes providing a unique and invigorating experience to selected fellows. It ensures the bes ...\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Prime Minister’s Fellowship for Doctoral Research scheme is a prestigious initiative of the Science and Engineering Research Board (SERB), Department of Science & Technology, Government of India towar ...\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Scheme Name: Prime Minister’s Fellowship for Doctoral Research\n",
      "Level: Central\n",
      "Category: Education & Learning, Science, IT & Communications\n",
      "Tags: Fellowship, Doctoral, Research, SERB, PhD, Fellow ...\n",
      "- KSCSTE Post-Doctoral Fellowship Programme : the quality of the proposal, presentation, discussion, details of publications, academic records/recognitions, etc. The Post-Doctoral Fellowship can be availed for research in an institution other tha ...\n",
      "- Abdul Kalam Technology Innovation National Fellowship : institutions in the country, so as to transfer fundamental research output in viable technological products/devices/components/processes. In brief, the principal objective of the Fellowship is to achi ...\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# --------------------------\n",
    "# 1️⃣ Load embeddings (same as used for precomputing)\n",
    "# --------------------------\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --------------------------\n",
    "# 2️⃣ Load existing Chroma collection\n",
    "# --------------------------\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"schemes_db\",          # your collection name\n",
    "    persist_directory=\"./chroma_store2\",   # directory where you saved vectors\n",
    "    embedding_function=embeddings           # needed for query embedding\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# --------------------------\n",
    "# 3️⃣ Prepare your Prompt Template\n",
    "# --------------------------\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a friendly and knowledgeable assistant specialized in Indian government schemes.\n",
    "\n",
    "1. Greet the user first with: \"Hello! How may I assist you with government schemes today?\"\n",
    "\n",
    "2. Behavior:\n",
    "   - If the user's question is about one or more schemes listed in the provided context:\n",
    "       - Provide a **structured answer for each relevant scheme**.\n",
    "       - Include the following metadata for every scheme if available:\n",
    "           - **Scheme Name**  \n",
    "           - **Eligibility**  \n",
    "           - **Benefits**  \n",
    "           - **Application Process**  \n",
    "           - **Required Documents**  \n",
    "           - **Validity / Duration**  \n",
    "           - **Level (Central/State)**  \n",
    "           - **Scheme Category**  \n",
    "           - **Tags**  \n",
    "       - Present each scheme clearly, like a **numbered mini-report**.\n",
    "       - If a field is missing, mention \"Not available\".\n",
    "\n",
    "   - If the user's question is **not related** to any scheme in the context:\n",
    "       - Respond politely: \"Certainly, I don't know the answer to that. Please enter a valid question related to government schemes.\"\n",
    "\n",
    "3. Always base your answer **only on the provided context**.  \n",
    "   - Do not make up information.\n",
    "   - If the context does not have enough details, say: \"Not enough information.\"\n",
    "\n",
    "Here is the information you can use:\n",
    "\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Answer in structured, multi-scheme format:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 4️⃣ Initialize local LLM\n",
    "# --------------------------\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-small\",  # CPU-friendly local model\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --------------------------\n",
    "# 5️⃣ Create RetrievalQA chain\n",
    "# --------------------------\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 6️⃣ Query example\n",
    "# --------------------------\n",
    "query = \"Tell me all information about Prime Minister’s Fellowship for Doctoral Research\"\n",
    "result = qa_chain.invoke({\"query\": query})  # use invoke() instead of deprecated __call__\n",
    "\n",
    "print(\"Answer:\\n\", result['result'])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(\"-\", doc.metadata['scheme_name'], \":\", doc.page_content[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5122b3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19288\\3130748084.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19288\\3130748084.py:12: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n",
      "c:\\Users\\MISTY ROY\\OneDrive\\Desktop\\Pythonfiles_vscode\\rag_proj\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MISTY ROY\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "C:\\Users\\MISTY ROY\\AppData\\Local\\Temp\\ipykernel_19288\\3130748084.py:53: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Scheme Name - Eligibility - Benefits - Application Process - Required Documents - Validity / Duration - Level (Central/State) - Category - Tags\n",
      "\n",
      "Sources:\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Features of the Fellowship: In addition to the attractive scholarship, the Prime Minister’s Fellowship emphasizes providing a unique and invigorating experience to selected fellows. It ensures the bes ...\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Prime Minister’s Fellowship for Doctoral Research scheme is a prestigious initiative of the Science and Engineering Research Board (SERB), Department of Science & Technology, Government of India towar ...\n",
      "- Prime Minister’s Fellowship for Doctoral Research : Scheme Name: Prime Minister’s Fellowship for Doctoral Research\n",
      "Level: Central\n",
      "Category: Education & Learning, Science, IT & Communications\n",
      "Tags: Fellowship, Doctoral, Research, SERB, PhD, Fellow ...\n",
      "- KSCSTE Post-Doctoral Fellowship Programme : the quality of the proposal, presentation, discussion, details of publications, academic records/recognitions, etc. The Post-Doctoral Fellowship can be availed for research in an institution other tha ...\n",
      "- Abdul Kalam Technology Innovation National Fellowship : institutions in the country, so as to transfer fundamental research output in viable technological products/devices/components/processes. In brief, the principal objective of the Fellowship is to achi ...\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Load embeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Load Chroma vector store\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"schemes_db\",\n",
    "    persist_directory=\"./chroma_store2\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# 3. Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"Hello! How may I assist you with government schemes today?\n",
    "\n",
    "Answer ONLY using the provided context. \n",
    "If context is insufficient, reply: \"Not enough information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Answer in structured format:\n",
    "- Scheme Name\n",
    "- Eligibility\n",
    "- Benefits\n",
    "- Application Process\n",
    "- Required Documents\n",
    "- Validity / Duration\n",
    "- Level (Central/State)\n",
    "- Category\n",
    "- Tags\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 4. Initialize local model (seq2seq style)\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",       # FIXED here\n",
    "    model=\"google/flan-t5-base\",  # use base for longer context\n",
    "    max_length=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 5. RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 6. Query\n",
    "query = \"Tell me all information about Prime Minister’s Fellowship for Doctoral Research\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\\n\", result['result'])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(\"-\", doc.metadata['scheme_name'], \":\", doc.page_content[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722973a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
